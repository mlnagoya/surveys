# Is Vibe Coding Safe? Benchmarking Vulnerability of Agent-Generated Code in Real-World Tasks

Songwen Zhao, Danqing Wang, Kexun Zhang, Jiaxuan Luo, Zhuo Li, Lei Li

https://arxiv.org/abs/2512.03262v1

## どんなもの？
- 実際の OSS のリポジトリで脆弱性を修正したコミットを元に作ったベンチマークデータセット SUSVIBES を作成
- SUSVIBES 上で LLM ベースのコード生成ツール (e.g. Claude Code) が生成したコードの機能性とセキュリティを評価
- 生成したコードの80%以上にセキュリティ上の脆弱性が存在することを確認
- セキュリティ上の問題点をプロンプトに入れても生成したコードのセキュリティは向上しないことも確認

## 先行研究と比べてどこがすごい？

- コーディングエージェントの研究が盛んに行われている。エージェントの振る舞いモデルそのものの訓練方法などの研究が多いが、生成されたコードのセキュリティに関する研究は少ない
- LLM が生成するコードのセキュリティを評価するためのベンチマークデータセットは既存のものもいくつか存在する。
  - 初期のものは単一関数、単一ファイルで構成される小規模なコードが多い
  - 最近ではより複雑な依存関係のあるリポジトリ単位のベンチマークも提案されているが、規模や網羅性に課題がある

![](is_vibe_coding_safe/comp_or_benchmarks.png)

## 技術や手法のキモはどこ？

### SUSVIBES データセットの構築

以下のような手順で SUSVIBES データセットを構築

1. OSS のリポジトリから脆弱性修正コミットを 20,000 件収集
  - リポジトリは Python に限る
  - テストコードの修正も同時に行われているものに限る
2. 各コミットの1つ前のコミットから、さらに、修正対象の機能自体を削除
  - このプロセスは実験対象とは別のコーディングエージェントを使って検証し、必要に応じてマスク範囲を拡大
3. 同時に、修正後のコードを生成するためのプロンプトも作成
  - プロンプトにはセキュリティに注意を促すコメントも入れている
4. これにより、以下が得られる
  - 脆弱性を含むコード (修正前のコミット)
  - 機能を実装するためのプロンプト
  - 脆弱性が修正されたコード (正解データ)
  - セキュリティを検証するためのテストコード (修正後のコミットで修正されたテストコード)
  - 機能を検証するためのテストコード (修正前のコミットにすでに存在するテストコード)

![](is_vibe_coding_safe/curation_pipeline.png)

実際のタスクの例

![](is_vibe_coding_safe/example_task.png)


## どうやって有効だと検証した？

3つのコーディングエージェント (SWE-Agent, OpenHands, Claude Code) 3つの LLM モデル (Kimi K2, Gemini 2.5 Pro, Claude 4 Sonnet) を使って SUSVIBES 上でコード生成を実行し、以下の2つの観点で評価

1. FUNC_PASS 生成されたコードが機能テストをパスする割合
2. SEC_PASS 生成されたコードがセキュリティテストをパスする割合

![](is_vibe_coding_safe/result1.png)

機能性が最も高い SWE-Agent + Claude Sonnet 4 (機能性評価61%) でもセキュリティ評価はわずか 10.5% に過ぎない。

### 緩和戦略

セキュリティに対するプロンプトを変えるとどうか？

- Self-selection ... 実装前にセキュリティリスクの検証を行うよう促す
- Oracle ... 実際にあった脆弱性をプロンプトに追加

![](is_vibe_coding_safe.png)

プロンプトに追加しても機能性が低下する上にセキュリティは向上しない。

## 議論はある？


## 次に読むべき論文は？
